<!DOCTYPE html>
<html lang="zh" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Young的作品集与博客</title>
<meta name="keywords" content="监督学习, 线性模型, 分类">
<meta name="description" content="1.算法总体
逻辑回归通过将线性回归的输出映射到概率空间来预测事件发生给概率：
$$
P(y=1|x;\theta)=h_{\theta}(x)=\frac{1}{1&#43;e^{-\theta^Tx}}
$$
该公式表示：在模型参数为$\theta$，输入特征值为$x$的情况下，分类标签$y=1$的概率为$h_{\theta}(x)$。
该模型一次推理的伪代码大致如下：
算法：LogisticForward
输入：特征值$x$
参数：向量$\theta$
输出：数据为正类别的概率值$P(y=1|x;\theta)$

计算线性组合$z = \theta^T x$
应用Sigmoid函数：$p=\sigma(z)=1/(1&#43;e^{-z})$
分类决策：若$p\leq 0.5$，则$\hat{y}=1$，否则$\hat{y}=0$
返回$\hat{y}$

该模型通过梯度下降的训练过程伪代码大致如下：
算法：LogisticTrainWithGD
输入：训练集$(X, y)$，其中$X\in \mathbb{R}^{m\times n}$
超参数：学习率$\alpha$，迭代次数$T$
输出：参数向量$\theta$

初始化参数$\theta$  # 初始化可以全0，也可以正态分布随机初始化等方法
对于$t=1$到$T$：
1. 初始化梯度$g\leftarrow 0$
2. 对于$i=1$到$m$：
1. $p_i \leftarrow$ LogisticForward($x_i$, $\theta$)
2. $g \leftarrow g&#43;(p_i-y_i)\cdot x_i$   # 计算新梯度
3. 更新参数：$\theta \leftarrow \theta - \frac{\alpha}{m}\cdot g$  # 梯度下降更新，详见目标函数部分
返回$\theta$

要使用其他方法更新，用对应的公式替代计算梯度和更新参数的两个步骤即可

实践过程中通常表现为直接修改对应物理地址的值，而非返回值

2.目标函数
2.1 目标函数公式
给定训练数据$(x_i, y_i)$，目标函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_ilog(h_{\theta}(x_i))&#43;(1-y_i)log(1-h_{\theta}(x_i))]
$$
其中$h_{\theta}$是Sigmoid函数：
$$
h_{\theta}(x)=\frac{1}{1&#43;e^{-\theta^Tx}}
$$
Sigmoid函数可用于将输出值转化为二分类概率值，多分类问题则使用Softmax函数。
">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/zh/notes/supervised/logistic/logistic/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.615b400c0435318d58c999397a3461ed5f0174fc2c426b65effcabf75bc7ccf8.css" integrity="sha256-YVtADAQ1MY1YyZk5ejRh7V8BdPwsQmtl7/yr91vHzPg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/zh/notes/supervised/logistic/logistic/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
          throwOnError : false
        });
    });
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/zh/" accesskey="h" title="Young的作品集与博客 (Alt + H)">Young的作品集与博客</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                    <ul class="lang-switch"><li>|</li>
                        <li>
                            <a href="http://localhost:1313/en/" title="English"
                                aria-label="English">En</a>
                        </li>
                    </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/zh/portfolio/" title="作品集">
                    <span>作品集</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/zh/blog/" title="博客">
                    <span>博客</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/zh/notes/" title="笔记">
                    <span>笔记</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/zh/resources/" title="资源">
                    <span>资源</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/zh/">主页</a>&nbsp;»&nbsp;<a href="http://localhost:1313/zh/notes/"></a>&nbsp;»&nbsp;<a href="http://localhost:1313/zh/notes/supervised/"></a></div>
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta"><span>4 分钟</span>

</div>
  </header> 
  <div class="post-content"><h2 id="1算法总体">1.算法总体<a hidden class="anchor" aria-hidden="true" href="#1算法总体">#</a></h2>
<p>逻辑回归通过将<strong>线性回归的输出</strong>映射到<strong>概率空间</strong>来预测事件发生给概率：
$$
P(y=1|x;\theta)=h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
该公式表示：在模型参数为$\theta$，输入特征值为$x$的情况下，分类标签$y=1$的概率为$h_{\theta}(x)$。
该模型一次<strong>推理</strong>的伪代码大致如下：</p>
<p><strong>算法：LogisticForward</strong>
输入：特征值$x$
参数：向量$\theta$
输出：数据为正类别的概率值$P(y=1|x;\theta)$</p>
<ol>
<li>计算线性组合$z = \theta^T x$</li>
<li>应用Sigmoid函数：$p=\sigma(z)=1/(1+e^{-z})$</li>
<li>分类决策：若$p\leq 0.5$，则$\hat{y}=1$，否则$\hat{y}=0$</li>
<li>返回$\hat{y}$</li>
</ol>
<p>该模型通过<strong>梯度下降</strong>的<strong>训练过程</strong>伪代码大致如下：</p>
<p><strong>算法：LogisticTrainWithGD</strong>
输入：训练集$(X, y)$，其中$X\in \mathbb{R}^{m\times n}$
超参数：学习率$\alpha$，迭代次数$T$
输出：参数向量$\theta$</p>
<ol>
<li>初始化参数$\theta$  # 初始化可以全0，也可以正态分布随机初始化等方法</li>
<li>对于$t=1$到$T$：
1. 初始化梯度$g\leftarrow 0$
2. 对于$i=1$到$m$：
1. $p_i \leftarrow$ <strong>LogisticForward($x_i$, $\theta$)</strong>
2. $g \leftarrow g+(p_i-y_i)\cdot x_i$   # 计算新梯度
3. 更新参数：$\theta \leftarrow \theta - \frac{\alpha}{m}\cdot g$  # 梯度下降更新，<strong>详见目标函数部分</strong></li>
<li>返回$\theta$</li>
</ol>
<p>要使用其他方法更新，用对应的公式替代计算梯度和更新参数的两个步骤即可</p>
<blockquote>
<p>实践过程中通常表现为直接修改对应物理地址的值，而非返回值</p>
</blockquote>
<h2 id="2目标函数">2.目标函数<a hidden class="anchor" aria-hidden="true" href="#2目标函数">#</a></h2>
<h3 id="21-目标函数公式">2.1 目标函数公式<a hidden class="anchor" aria-hidden="true" href="#21-目标函数公式">#</a></h3>
<p>给定训练数据$(x_i, y_i)$，目标函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_ilog(h_{\theta}(x_i))+(1-y_i)log(1-h_{\theta}(x_i))]
$$
其中$h_{\theta}$是Sigmoid函数：
$$
h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
Sigmoid函数可用于将输出值转化为二分类概率值，多分类问题则使用Softmax函数。
<img alt="Sigmoid和Softmax函数的作用" loading="lazy" src="imgs/lgstr.png"></p>
<p>该目标函数也叫做<strong>逻辑损失(统计学角度)</strong> ，在二分类中等价于 <strong>交叉熵损失(信息论角度)</strong>，可通过Softmax扩展到多分类问题。拟合目标是<strong>最小化目标函数</strong>。</p>
<p>目标函数性质如下</p>
<table>
  <thead>
      <tr>
          <th>性质名称</th>
          <th>性质</th>
          <th>注释</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>凸性</strong></td>
          <td>凸，非强凸</td>
          <td>唯一解具有理论保障，可以通过增加二次正则化项保证其强凸以加速收敛</td>
      </tr>
      <tr>
          <td><strong>光滑性</strong></td>
          <td>满足Lipschitz条件</td>
          <td>无穷可微，可以理论分析收敛速度</td>
      </tr>
  </tbody>
</table>
<h3 id="22-参数的梯度下降更新">2.2 参数的梯度下降更新<a hidden class="anchor" aria-hidden="true" href="#22-参数的梯度下降更新">#</a></h3>
<blockquote>
<p>下降方法是一种一阶优化方法，其原理详见[[梯度下降及其收敛性|一阶方法-梯度下降]]</p>
</blockquote>
<p>目标函数的<strong>梯度</strong>如下，计算过程略：
$$
\nabla J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)
$$
一般梯度下降中，以如下的方式更新参数$\theta$：
$$
\theta \leftarrow \theta - \alpha \nabla J(\theta)
$$
其中$\alpha$是学习率，也就是更新的步长，$\alpha$越大，更新越快，但也更容易错过最优点。$\leftarrow$为赋值符号，作用等同于Python中的等号。</p>
<blockquote>
<p>需要注意的是，许多的更新方法都是隐式地最小化目标函数，实际代码中损失值并未参与计算，而是作为一种评价指标。</p>
</blockquote>
<h3 id="23-参数拟牛顿法更新">2.3 参数拟牛顿法更新<a hidden class="anchor" aria-hidden="true" href="#23-参数拟牛顿法更新">#</a></h3>
<blockquote>
<p>拟牛顿法是一种介于梯度下降（一阶）和牛顿法（二阶）之间的优化方法，其原理详见[[牛顿法|二阶方法-牛顿法]]</p>
</blockquote>
<p>参数的拟牛顿法更新方式如下：
$$
\theta_{k+1} \leftarrow \theta_{k} - \alpha_kB_k^{-1}\nabla J(\theta_k)
$$
其中$B_k^{-1}$是$\theta_k$的Hessian矩阵近似。矩阵近似有很多种计算方法，此处介绍BFGS拟牛顿法。</p>
<p>$$
H_{k+1} \leftarrow H_k + \frac{ss^T}{s^Ty} - \frac{H_k yy^T H_k}{y^T H_k y}
$$
其中，$s=\theta_{k+1} - \theta_k$，详细推导过程略（因为我懒得学了好麻烦）
该方法可以避免求二阶段导导致计算复杂度进一步增大</p>
<p>Hessian矩阵近似可以通过如下的方式初始化：</p>
<ul>
<li>※※※ 单位矩阵初始化</li>
<li>以$\beta$倍缩放后的单位矩阵初始化</li>
<li>根据特定任务自定义初始化</li>
</ul>
<h3 id="24-带正则化项的逻辑回归函数">2.4 带正则化项的逻辑回归函数<a hidden class="anchor" aria-hidden="true" href="#24-带正则化项的逻辑回归函数">#</a></h3>
<blockquote>
<p>这个例子在[[强凸性与光滑性#1.4.2逻辑回归(Logistic Regression)+L2正则化|强凸性笔记]]有所介绍，具体的强凸性证明请参照对应章节</p>
</blockquote>
<p>根据强凸性定义，在目标函数后增加二次函数，并加以变换，可以使得目标函数具有强凸性，变化后的形式如下：
$$
J(w) = \frac{1}{n}\sum_{i=1}^{n}log(1+e^{(-y_iw^Tx_i)}) + \frac{\lambda}{2}|w^2|
$$
此时目标函数是  $\lambda$-强凸  的</p>
<h2 id="3算法实现">3.算法实现<a hidden class="anchor" aria-hidden="true" href="#3算法实现">#</a></h2>
<p>我们使用如下的方式获取用于测试的分类数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_classification
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成分类数据</span>
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">114514</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, n_class<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, n_informative<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1919810</span>, n_clusters_per_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 划分数据集</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(
</span></span><span style="display:flex;"><span>	X, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1919810</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="31-傻瓜式实现">3.1 傻瓜式实现<a hidden class="anchor" aria-hidden="true" href="#31-傻瓜式实现">#</a></h3>
<blockquote>
<p>实际应用中就这样实现，<strong>不要自己搓轮子了</strong></p>
</blockquote>
<p><strong>sklearn调用<code>LogisticRegression</code>实现算法</strong></p>
<blockquote>
<p>sklearn是相当经典的行运算库，适合研究，实现idea等小规模数据分析</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression  <span style="color:#75715e"># 直接引入逻辑回归方法</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LogisticRegression(
</span></span><span style="display:flex;"><span>	penalty<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;None&#39;</span>,  <span style="color:#75715e"># 无正则化，和上文的公式一致，设置为L2可将目标变得强凸</span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># C=1.0,   # 只有在启动正则化后才有效，数字越小正则化强度越大</span>
</span></span><span style="display:flex;"><span>	max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,  <span style="color:#75715e"># 最大迭代次数</span>
</span></span><span style="display:flex;"><span>	solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;saga&#39;</span>,  <span style="color:#75715e"># 梯度下降求解器，模型默认实际上是L-BFGS，更高性能的BFGS法</span>
</span></span><span style="display:flex;"><span>	random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1919810</span>  <span style="color:#75715e"># 随机种子</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)  <span style="color:#75715e"># 预测，输出预测标签值数组</span>
</span></span><span style="display:flex;"><span>y_prob_pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict_proba(X_test)  <span style="color:#75715e"># 输出预测标签概率数组</span>
</span></span></code></pre></div><p>注意：参数$C$实际上是强突定义中$\lambda$的倒数，即有：
$$
C = \frac{1}{\lambda}
$$
$C$越小，目标函数的强凸性越大，强凸中的$\frac{1}{2}$不会参与到倒数运算中，这一点会在[[逻辑回归#3.2 逐步实现(本地Python，无高级api)|逐步实现]]中得到展示</p>
<p><strong>PySpark调用<code>LogisticRegression</code></strong></p>
<blockquote>
<p>PySpark是Spark的Python接口，其以列为操作单位，适合海量数据处理，不适合用于构建更加精密复杂、带有创新的分类器。编写PySpark代码时应当注重对数据的批量操作(整个DataFrame或列)</p>
</blockquote>
<blockquote>
<p>推荐使用PySpark3.0以上版本</p>
</blockquote>
<p>假设有如下格式的DataFrame变量 <code>traindata</code>和<code>testdata</code></p>
<table>
  <thead>
      <tr>
          <th>x_vec: Vector</th>
          <th>y_vec: Vector</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>[1, 1, 1, 1, 1]</td>
          <td>1</td>
      </tr>
      <tr>
          <td>[0, 1, 0, 1, 1]</td>
          <td>2</td>
      </tr>
      <tr>
          <td>&hellip;</td>
          <td>&hellip;</td>
      </tr>
  </tbody>
</table>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.ml.classification <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder \  <span style="color:#75715e"># 创建Spark会话以与集群资源/本地资源互动</span>
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">.</span>appName(<span style="color:#e6db74">&#34;LRDemo&#34;</span>) \
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">.</span>getOrGreate()
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这是一个Estimator子类，能定义模型参数并通过.fit生成带训练参数的Transformer子类</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression(
</span></span><span style="display:flex;"><span>	featuresCol<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x_vec&#34;</span>,  <span style="color:#75715e"># 选中特征列</span>
</span></span><span style="display:flex;"><span>	labelCol<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;y_vec&#34;</span>,  <span style="color:#75715e"># 选中标签列</span>
</span></span><span style="display:flex;"><span>	maxIter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,  <span style="color:#75715e"># 最大迭代次数</span>
</span></span><span style="display:flex;"><span>	family<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binomial&#34;</span>,  <span style="color:#75715e"># 问题是二分类</span>
</span></span><span style="display:flex;"><span>	regParam<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>  <span style="color:#75715e"># 正则化强度，作用等同于sklearn的C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这是一个Transformer子类，它只能根据Estimator子类给出的训练参数进行推理</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>fit(train_data)  <span style="color:#75715e"># PySpark会生成新的变量，而非像sklearn那样修改原有变量</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>transform(test_data)  <span style="color:#75715e"># 模型推理</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># columns = [x_vec, y_vec, prediction]</span>
</span></span></code></pre></div><h3 id="32-逐步实现本地python无高级api">3.2 逐步实现(本地Python，无高级api)<a hidden class="anchor" aria-hidden="true" href="#32-逐步实现本地python无高级api">#</a></h3>
<p><strong>目标函数（损失函数）计算</strong>
我们只需要实现单个样本的计算方式然后求均值即可，其公式为：
$$
y_ilog(h_{\theta}(x_i)))+(1-y_i)log(1-h_{\theta}(x_i))
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sigmoid</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(z):
</span></span><span style="display:flex;"><span>	<span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	该函数用于实现h(x)，即Sigmoid函数
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param z: Sigmoid函数输入值，可以是标量或向量
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:return : Sigmoid函数输出值
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># np.where有三个参数，[判断条件/布尔值，条件为真的值，条件为假的值]</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(z <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>, 
</span></span><span style="display:flex;"><span>					<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z)),  <span style="color:#75715e"># 大于0时使用标准形式 </span>
</span></span><span style="display:flex;"><span>					np<span style="color:#f92672">.</span>exp(z)<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(z)))  <span style="color:#75715e"># 当小于0时使用等级啊变换避免溢出</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_loss</span>(theta, X, y):
</span></span><span style="display:flex;"><span>	<span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	计算标准的对数损失
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param theta: 算法参数，应当为数组，尺寸为(特征数, )，这也是我们要更新的部分
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param X: 数据的特征值，应当为数组，尺寸为(样本数, 特征数)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param y: 数据的标签值，应当为数组，尺寸为(样本数)，值必须为0或者1
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:return loss: 逻辑(二元交叉熵/对数)损失值
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	z <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> theta  <span style="color:#75715e"># 运算线性组合</span>
</span></span><span style="display:flex;"><span>	h <span style="color:#f92672">=</span> sigmoid(z)
</span></span><span style="display:flex;"><span>	epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-15</span>  <span style="color:#75715e"># 极小数，避免log0</span>
</span></span><span style="display:flex;"><span>	h <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>clip(h, epsilon, <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>epsilon)  <span style="color:#75715e"># 裁剪运算结果</span>
</span></span><span style="display:flex;"><span>	loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>mean(
</span></span><span style="display:flex;"><span>		y <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(h) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y) <span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> h)
</span></span><span style="display:flex;"><span>	)
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> loss
</span></span></code></pre></div><p>带正则项的目标函数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_loss_reg</span>(theta, X, y, lambda_reg):
</span></span><span style="display:flex;"><span>	<span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	计算带正则化项的损失
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param theta: 算法参数
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param X: 数据特征值
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param y: 数据标签值
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:param lambda_reg: 正则化强度，直接决定目标函数强凸程度，函数变化为 lambda_reg-强凸函数，取0则不进行正则化
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	:return : 带正则化项的对数损失
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>	base_loss <span style="color:#f92672">=</span> logistic_loss(theta, X, y)
</span></span><span style="display:flex;"><span>	reg_term <span style="color:#f92672">=</span> (lambda_reg <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> m)) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sum(theta[<span style="color:#ae81ff">1</span>:]<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># 正则化需要去除截距项，后同</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> base_loss <span style="color:#f92672">+</span> reg_term
</span></span><span style="display:flex;"><span>	
</span></span></code></pre></div><p>计算梯度</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_grad</span>(theta, X, y):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	n_samples <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>	z <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> theta
</span></span><span style="display:flex;"><span>	h <span style="color:#f92672">=</span> sigmoid(z)
</span></span><span style="display:flex;"><span>	gradient <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>n_samples) <span style="color:#f92672">*</span> X<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> (h<span style="color:#f92672">-</span>y)  <span style="color:#75715e"># 直接套用公式</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> gradient
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_grad_reg</span>(theta, X, y, lambda_reg):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	n_samples <span style="color:#f92672">=</span> len(y)
</span></span><span style="display:flex;"><span>	gradient <span style="color:#f92672">=</span> get_grad(theta, X, y)
</span></span><span style="display:flex;"><span>	reg_gradient[<span style="color:#ae81ff">1</span>:] <span style="color:#f92672">=</span> (lambda_reg <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> theta[<span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> gradient <span style="color:#f92672">+</span> reg_gradient
</span></span></code></pre></div><p>梯度下降更新</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient_descent</span>(X, y, theta_init<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-2</span>, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, lambda_reg<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>	<span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">	&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	m, n <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">if</span> theta_init <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>		theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros()
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>		theta <span style="color:#f92672">=</span> theta_init<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	loss_values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_iter):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> lambda_reg <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>			loss <span style="color:#f92672">=</span> logistic_loss_reg(theta, X, y, lambda_reg)
</span></span><span style="display:flex;"><span>			grad <span style="color:#f92672">=</span> get_grad_reg(theta, X, y, lambda_reg)
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>			loss <span style="color:#f92672">=</span> logistic_loss(theta, X, y)
</span></span><span style="display:flex;"><span>			grad <span style="color:#f92672">=</span> get_grad(theta, X, y)
</span></span><span style="display:flex;"><span>		loss_values<span style="color:#f92672">.</span>append(loss)
</span></span><span style="display:flex;"><span>		theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grad
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> theta, loss_values
</span></span></code></pre></div><p>所有代码的类实现</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LogisticRegression</span>:
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(
</span></span><span style="display:flex;"><span>		self,
</span></span><span style="display:flex;"><span>		learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-2</span>,
</span></span><span style="display:flex;"><span>		n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,
</span></span><span style="display:flex;"><span>		lambda_reg<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>		fit_intercept<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>	):
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>n_iter <span style="color:#f92672">=</span> n_iter
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>lambda_reg <span style="color:#f92672">=</span> lambda_reg
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>fit_intercept <span style="color:#f92672">=</span> fit_intercept
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>theta <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>loss_values <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_add_intercept</span>(self, X):
</span></span><span style="display:flex;"><span>		m <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>c_[np<span style="color:#f92672">.</span>ones(m), X]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, y):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>fit_intersept:
</span></span><span style="display:flex;"><span>			X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_add_intercept(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>theta, self<span style="color:#f92672">.</span>loss_values <span style="color:#f92672">=</span> gradient_descent(
</span></span><span style="display:flex;"><span>			X,
</span></span><span style="display:flex;"><span>			y,
</span></span><span style="display:flex;"><span>			learning_rate,
</span></span><span style="display:flex;"><span>			n_iter,
</span></span><span style="display:flex;"><span>			lambda_reg
</span></span><span style="display:flex;"><span>		)
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> self
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_proba</span>(self, X):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>theta <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(<span style="color:#e6db74">&#34;No parameters fitted yet!&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>fit_intercept:
</span></span><span style="display:flex;"><span>			X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_add_inetercept(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> sigmoid(X <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>theta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/zh/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a></li>
      <li><a href="http://localhost:1313/zh/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">线性模型</a></li>
      <li><a href="http://localhost:1313/zh/tags/%E5%88%86%E7%B1%BB/">分类</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on x"
            href="https://x.com/intent/tweet/?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f&amp;hashtags=%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0%2c%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b%2c%e5%88%86%e7%b1%bb">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f&amp;title=&amp;summary=&amp;source=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f&title=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on whatsapp"
            href="https://api.whatsapp.com/send?text=%20-%20http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on telegram"
            href="https://telegram.me/share/url?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=&u=http%3a%2f%2flocalhost%3a1313%2fzh%2fnotes%2fsupervised%2flogistic%2flogistic%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/zh/">Young的作品集与博客</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
