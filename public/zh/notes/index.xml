<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Notes on Young的作品集与博客</title>
    <link>http://localhost:1313/zh/notes/</link>
    <description>Recent content in Notes on Young的作品集与博客</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 09 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/zh/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>逻辑回归</title>
      <link>http://localhost:1313/zh/notes/supervised/logistic/</link>
      <pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/zh/notes/supervised/logistic/</guid>
      <description>&lt;h2 id=&#34;1算法总体&#34;&gt;1.算法总体&lt;/h2&gt;
&lt;p&gt;逻辑回归通过将&lt;strong&gt;线性回归的输出&lt;/strong&gt;映射到&lt;strong&gt;概率空间&lt;/strong&gt;来预测事件发生给概率：
$$
P(y=1|x;\theta)=h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
该公式表示：在模型参数为$\theta$，输入特征值为$x$的情况下，分类标签$y=1$的概率为$h_{\theta}(x)$。
该模型一次&lt;strong&gt;推理&lt;/strong&gt;的伪代码大致如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：LogisticForward&lt;/strong&gt;
输入：特征值$x$
参数：向量$\theta$
输出：数据为正类别的概率值$P(y=1|x;\theta)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算线性组合$z = \theta^T x$&lt;/li&gt;
&lt;li&gt;应用Sigmoid函数：$p=\sigma(z)=1/(1+e^{-z})$&lt;/li&gt;
&lt;li&gt;分类决策：若$p\leq 0.5$，则$\hat{y}=1$，否则$\hat{y}=0$&lt;/li&gt;
&lt;li&gt;返回$\hat{y}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该模型通过&lt;strong&gt;梯度下降&lt;/strong&gt;的&lt;strong&gt;训练过程&lt;/strong&gt;伪代码大致如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：LogisticTrainWithGD&lt;/strong&gt;
输入：训练集$(X, y)$，其中$X\in \mathbb{R}^{m\times n}$
超参数：学习率$\alpha$，迭代次数$T$
输出：参数向量$\theta$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化参数$\theta$  # 初始化可以全0，也可以正态分布随机初始化等方法&lt;/li&gt;
&lt;li&gt;对于$t=1$到$T$：
1. 初始化梯度$g\leftarrow 0$
2. 对于$i=1$到$m$：
1. $p_i \leftarrow$ &lt;strong&gt;LogisticForward($x_i$, $\theta$)&lt;/strong&gt;
2. $g \leftarrow g+(p_i-y_i)\cdot x_i$   # 计算新梯度
3. 更新参数：$\theta \leftarrow \theta - \frac{\alpha}{m}\cdot g$  # 梯度下降更新，&lt;strong&gt;详见目标函数部分&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;返回$\theta$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;要使用其他方法更新，用对应的公式替代计算梯度和更新参数的两个步骤即可&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;实践过程中通常表现为直接修改对应物理地址的值，而非返回值&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;2目标函数&#34;&gt;2.目标函数&lt;/h2&gt;
&lt;h3 id=&#34;21-目标函数公式&#34;&gt;2.1 目标函数公式&lt;/h3&gt;
&lt;p&gt;给定训练数据$(x_i, y_i)$，目标函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_ilog(h_{\theta}(x_i))+(1-y_i)log(1-h_{\theta}(x_i))]
$$
其中$h_{\theta}$是Sigmoid函数：
$$
h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
Sigmoid函数可用于将输出值转化为二分类概率值，多分类问题则使用Softmax函数。
&lt;img alt=&#34;Sigmoid和Softmax函数的作用&#34; loading=&#34;lazy&#34; src=&#34;imgs/lgstr.png&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
